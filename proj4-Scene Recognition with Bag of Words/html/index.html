<html>
<head>
<title>Recognition with Bag of Words</title>
<link href='http://fonts.googleapis.com/css?family=Nunito:300|Crimson+Text|Droid+Sans+Mono' rel='stylesheet' type='text/css'>
<link rel="stylesheet" title="Default" href="styles/github.css">
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script>  

<link rel="stylesheet" href="highlighting/styles/default.css">
<script src="highlighting/highlight.pack.js"></script>

<style type="text/css">
body {
	margin: 0px;
	width: 100%;
	font-family: 'Crimson Text', serif;
	font-size: 20px;
	background: #fcfcfc;
}
h1 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 28px;
	margin: 25px 0px 0px 0px;
	text-transform: lowercase;

}

h2 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 32px;
	margin: 15px 0px 35px 0px;
	color: #333;	
	word-spacing: 3px;
}

h3 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 26px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}
h4 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 22px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}

h5 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 18px;
	margin: 10px 0px 10px 0px;
	color: #111;
	word-spacing: 2px;
}

p, li {
	color: #444;
}

a {
	color: #DE3737;
}

.container {
	margin: 0px auto 0px auto;
	width: 1160px;
}

#header {
	background: #333;
	width: 100%;
}

#headersub {
	color: #ccc;
	width: 960px;
	margin: 0px auto 0px auto;
	padding: 20px 0px 20px 0px;
}

.chart {
	width: 480px;
}
.lol {
	font-size: 16px;
	color: #888;
	font-style: italic;
}
.sep {
	height: 1px;
	width: 100%;
	background: #999;
	margin: 20px 0px 20px 0px;
}
.footer{
	font-size: 16px;
}
.latex {
	width: 100%;
}

.latex img {
	display: block;
	margin: 0px auto 0px auto;
}

pre {
	font-family: 'Droid Sans Mono';
	font-size: 18px;
}

table td {
  text-align: center;
  vertical-align: middle;
}

table td img {
  text-align: center;
  vertical-align: middle;
}

#contents a {
}
</style>
<script type="text/javascript">
    hljs.initHighlightingOnLoad();
</script>
</head>
<body>
<div id="header" >
<div id="headersub">
<h1>Wenqi Xian</h1>
</div>
</div>
<div class="container">

<h2>Project 4 / Scene Recognition with Bag of Words</h2>

<p> 	
	The goal of this project is to recognize 15 categories of scene using different image representation and and classifier. 
	We then compare the performance between each model. My methods and best results are listed in the table below, and I will give a detailed analysis in the following sections:
</p>
<center>
<table border="1">
<tr>
<th>Image Representation</th>
<th>Classifier</th>
<th>My best result in accuracy(%)</th>
</tr>
<tr>
<td>Tiny images representation</td>
<td>Nearest neighbor classifier</td>
<td>20.2%</td>
</tr>
<tr>
<td>Bag of SIFT representation</td> 
<td>Nearest neighbor classifier</td>
<td>50.9%</td>
</tr>
<tr>
<td>Bag of SIFT representation</td> 
<td>Linear SVM classifier</td>
<td>63.8%</td>
</tr>
<tr>
<td>Bag of SIFT representation</td>
<td>Non-Linear SVM classifier</td>
<td>69%</td>
</tr>
<tr>
<td>GMM & Fisher encoding</td> 
<td>Non-linear SVM classifier</td>
<td>81.6%</td>
</tr>
<tr>
<td>GMM & Fisher encoding</td> 
<td>Linear SVM classifier</td>
<td>81.8%</td>
</tr>
</table>
</center>
<p><br></p>

<h4><u>Tiny image + Nearest neighbor</u></h4>
<p>Tiny image is a very simple image representation. I resized image to 16x16, and used them as a features. 
	I implemented 1-Nearest neighbor. We compare each feature of test image with every image in training data set and predict the label of the nearest one.</p>
<p>My first attempt of using tiny image with nearest neighbor classifier yeilds result of 19.1%. This is definitely an improvement from 7% chance. 
	After normalizing tiny image with zero mean and unit length, the accuracy is improved to 20.2% with the vocab size of 50.</p>
<center>
<img src="results_saved/tiny+1nn(1)/confusion_matrix.png" height="400" width="500"/>
<p style="font-size: 18px">Confusion matrix for tiny image representation (accuracy: 20.2%)
<a style="font-size: 18px" href="results_saved/tiny+1nn(1)/index.html">detailed result visualization</a>
</p>
</center>

<h4><u>Bag of SIFT + Nearest neighbor</u></h4>

<p>I used SIFT descriptors to build a vocabulary of 'visual words' and constructed histograms to represent the frequency of the visual words appear on each image. 
	First, to do build the vocabulary, I extracted the SIFT features from each training image using vl_dsift.
	Then, I build a vocabulary of 'visual words' by clustering the SIFT features using Kmeans.
	Finally, I represent each image as a histogram of word frequency.
</p>
<p> I improved the accuracy from 48% to 49.6% by normalizing the histogram(vocab size of 50). 
	I also further optimized the design by tuning the step size and bin size of SIFT descriptors, and there is a detailed summary in the next section.
	In this method, I used step size of 15 to build vocabulary and step size of 5 to get bag of SIFT. The final result with vocab size of 200 is 50.9%.
</p>

<center>
<img src="results_saved/bag+1nn(1)/confusion_matrix.png" height="400" width="500"/>
<p style="font-size: 18px">Confusion matrix for Bag of SIFT + Nearest neighbor(accuracy: 50.9%)
<a style="font-size: 18px" href="results_saved/bag+1nn(1)/index.html">detailed result visualization</a>
</p>
</center>

<h4><u>Bag of SIFT + Linear SVM</u></h4>

<p>This time, I trained the set of sifts features on linear SVM which calculates a weight(w) and bias(b) for each category (vl_svmtrain).
	Then I computed the confidence weight for each test image by plugging its feature vector into w*x+b. For each image, the category is determined by the highest confidence. 
	Initially, I used a relatively large lambda, and it is proven that smaller lambda tends to give better results. 
	Finally, I set 0.000001 as the value for lambda, yeilding accuracy of 63.8%.
</p>

<center>
<img src="results_saved/bag+svm(1)/confusion_matrix.png" height="400" width="500"/>
<p style="font-size: 18px">Confusion matrix for Bag of SIFT + Linear SVM(accuracy: 63.8%)
<a style="font-size: 18px" href="results_saved/bag+svm(1)/index.html">detailed result visualization</a>
</p>
</center>

<h1> Extra Credit </h1>
<h4><u>Parameterization</u></h4>
<p>The classification accuracy is greatly effected by the choice of the parameters for finding features, clustering visual words and training the SVMs.<br>
 The paramters that worked best for me are listed in the following table. Since there is a trade-off between accuracy and computation speed when classfying a large set of images,
 I chose parameters that yielded reasonable results and did not take too much time to compute (take around 5 minutes to compute in most cases).</p>

<center>
	<table border="1">
		<tr>
			<th>Step</th>
			<th>Free Parameter</th>
			<th>Value</th>
		</tr>
		<tr>
			<td>build_vocabulary</td>
			<td>Vocabulary size</td>
			<td>200</td>
		</tr>
		<tr>
			<td></td>
			<td>SIFT step size</td>
			<td>15</td>
		</tr>
		<tr>
			<td></td>
			<td>SIFT bin step</td>
			<td>8</td>
		</tr>
		<tr>
			<td>get_bag_of_sifts</td>
			<td>SIFT step size</td>
			<td>5</td>
		</tr>
		<tr>
			<td></td>
			<td>SIFT bin step</td>
			<td>8</td>
		</tr>
		<tr>
			<td>svm_classify</td>
			<td>lambda</td>
			<td>0.000001</td>
		</tr>
	</table>
</center>

<h4>Vocabulary Size</h4>
<div style="text-align: center;">
<img src="accuracy_chart.png" style="display: inline-block;" height="350" width="650" />
</div>
The figure above shows the impact of the vocabulary size. While too small and too large vocabulary sizes lead to inaccurate classifications, choosing a size between 200 and 300 yield the best results.
<p></p>	

<h4><u>Bag of SIFT + Non-linear SVM</u></h4>

<p>
Now, it's time to use more sophisticated kernels instead of just linear svm. I tried RBF kernel using Olivier Chapelle's MATLAB implementation of primal_svm. 
With the same parameters and vocab size(200) as before, rbf kernel improved the accuracy to 69% with sigma = 1, which is a reasonable improvement from linear svm of 63.8%.

<center>
<img src="results_saved/bag+rbf(1)/confusion_matrix.png" height="400" width="500"/>
<p style="font-size: 18px">Confusion matrix for bag of SIFT + Non-linear SVM(accuracy: 69%)
<a style="font-size: 18px" href="results_saved/bag+rbf(1)/index.html">detailed result visualization</a>
</p>
</center>

<h4><u>GMM & Fisher encoding + Non-linear SVM</u></h4>

<p> Now it's time to optimize the design by addressing the quantization error.
	I used GMM and fisher encoding, a more sophisticated feature encoding schemes analyzed in the comparative study of Chatfield et al.
	The Fisher Vector representation of visual features is an extension of the bag of visual words. 
	The previous method uses K-Means to generate the feature vocabulary, 
	where as now we uses Gaussian Mixture Models (GMMs) to do the same. 
	So instead of constructing a hard coded codebook, we generate a probabilistic visual vocabulary. 
	And then we can compute the gradient of the log likelihood with respect to the parameters of the model to represent an image or video. 
	The Fisher Vector is the concatenation of these partial derivatives and describes in which direction the parameters of the model should be modified to best fit the data. 
	Therefore, This method gives better classification performance than BOV obtained with Kmeans.</p>

<p>	First, I computed the statistics of training data using vl_gmm, and then computed the fisher vector signature for each image using vl_fisher.
	With the same rbf svm and parameters as before, the accuracy of using fisher encoding is 74.4%, which is a great improvement from 69% of BOW. 
	The vocab size is still 50. Later, I increased the vocab size to 200, and get an accuracy of 77.6%.
</p>

<center>
<img src="results_saved/fisher+svm(4)/confusion_matrix.png" height="400" width="500"/>
<p style="font-size: 18px">Confusion matrix for fisher encoding + Non-linear SVM(accuracy: 77.6%)
<a style="font-size: 18px" href="results_saved/fisher+svm(4)/index.html">detailed result visualization</a>
</p>
</center>
<p>
I also apply power normalization and L2-normalization on the resulting fisher vectors, which boosts the result to 81.6%.
Feel free to test the program by uncommenting %FEATURE = 'fisher encoding'; on line 21 in proj4.m.
</p>

<center>
<img src="results_saved/fisher+rbf(1)/confusion_matrix.png" height="400" width="500"/>
<p style="font-size: 18px">Confusion matrix for fisher encoding(normalized) + Non-linear SVM(accuracy: 81.6%)
<a style="font-size: 18px" href="results_saved/fisher+rbf(1)/index.html">detailed result visualization</a>
</p>
</center>

<h4><u>GMM & Fisher encoding + Linear SVM</u></h4>

<p> Fisher encoding even performs well with simple linear classifiers. The best accuracy I got so far is 81.8%.
	If we have a good model like Fisher encoding, a simple linear classifier will work just fine.
	Where as non-linear SVM tends to add unwanted complexity, or starts to overfit our data!
	A significant benefit of linear classifiers is that they are very efficient to learn and easy to evaluate.</p>

<center>
<img src="results_saved/fisher+svm(5)/confusion_matrix.png" height="400" width="500"/>
<p style="font-size: 18px">Confusion matrix for fisher encoding + Linear SVM(accuracy: 81.8%)
<a style="font-size: 18px" href="results_saved/fisher+svm(5)/index.html">detailed result visualization</a>
</p>
</center>
</body>
</html>
